version: '3.8'

services:
  # API Principal - Múltiplas instâncias para alta disponibilidade
  api:
    build: 
      context: .
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WHATSAPP_ACCESS_TOKEN=${WHATSAPP_ACCESS_TOKEN}
      - WHATSAPP_PHONE_NUMBER_ID=${WHATSAPP_PHONE_NUMBER_ID}
      - WHATSAPP_BUSINESS_ACCOUNT_ID=${WHATSAPP_BUSINESS_ACCOUNT_ID}
    depends_on:
      - postgres-master
      - redis-cluster
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # WebSocket Server - Múltiplas instâncias
  websocket:
    build: 
      context: .
      dockerfile: Dockerfile
    command: uvicorn app.websocket.main:app --host 0.0.0.0 --port 8001 --workers 2
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - postgres-master
      - redis-cluster
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Celery Workers - Processamento assíncrono
  worker:
    build: 
      context: .
      dockerfile: Dockerfile
    command: celery -A app.workers.main worker --loglevel=info --concurrency=8 --max-tasks-per-child=1000
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WHATSAPP_ACCESS_TOKEN=${WHATSAPP_ACCESS_TOKEN}
    depends_on:
      - postgres-master
      - redis-cluster
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # Celery Beat - Scheduler
  scheduler:
    build: 
      context: .
      dockerfile: Dockerfile
    command: celery -A app.workers.main beat --loglevel=info --schedule=/tmp/celerybeat-schedule
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
    depends_on:
      - postgres-master
      - redis-cluster
    volumes:
      - ./logs:/app/logs
      - scheduler_data:/tmp
    restart: unless-stopped

  # PostgreSQL Master (Write)
  postgres-master:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: isp_support
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
      POSTGRES_REPLICATION_MODE: master
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${POSTGRES_REPLICATION_PASSWORD}
    volumes:
      - postgres_master_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL Slave (Read Replica)
  postgres-slave:
    image: postgres:15-alpine
    environment:
      POSTGRES_MASTER_SERVICE: postgres-master
      POSTGRES_REPLICATION_MODE: slave
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${POSTGRES_REPLICATION_PASSWORD}
      POSTGRES_MASTER_PORT_NUMBER: 5432
    depends_on:
      - postgres-master
    volumes:
      - postgres_slave_data:/var/lib/postgresql/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  # Redis Cluster - Cache distribuído
  redis-cluster:
    image: redis:7-alpine
    command: redis-server /usr/local/etc/redis/redis.conf --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes
    volumes:
      - redis_cluster_data:/data
      - ./config/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  # HAProxy Load Balancer
  haproxy:
    image: haproxy:2.8-alpine
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # Stats
    volumes:
      - ./config/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./config/ssl:/etc/ssl/certs
    depends_on:
      - api
      - websocket
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

  # Nginx - Servidor de arquivos estáticos e proxy reverso
  nginx:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./config/nginx-production.conf:/etc/nginx/nginx.conf
      - ./static:/usr/share/nginx/html/static
      - ./uploads:/usr/share/nginx/html/uploads
    depends_on:
      - haproxy
    restart: unless-stopped

  # Prometheus - Monitoramento
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus-production.yml:/etc/prometheus/prometheus.yml
      - ./config/prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: unless-stopped

  # Grafana - Dashboards
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped

  # AlertManager - Alertas
  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    restart: unless-stopped

  # Elasticsearch - Logs
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - cluster.name=isp-logs
      - node.name=es-node-1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

  # Kibana - Visualização de logs
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Logstash - Processamento de logs
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    volumes:
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline
      - ./config/logstash/config:/usr/share/logstash/config
      - ./logs:/usr/share/logstash/logs
    environment:
      - "LS_JAVA_OPTS=-Xmx1g -Xms1g"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Filebeat - Coleta de logs
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    user: root
    volumes:
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/app:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Backup Service
  backup:
    image: postgres:15-alpine
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./backups:/backups
      - ./scripts/backup.sh:/backup.sh
    command: |
      sh -c "
        while true; do
          sleep 86400  # 24 horas
          /backup.sh
        done
      "
    depends_on:
      - postgres-master
    restart: unless-stopped

  # Watchtower - Auto-update containers
  watchtower:
    image: containrrr/watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 2 * * *  # 2 AM daily
    restart: unless-stopped

volumes:
  postgres_master_data:
    driver: local
  postgres_slave_data:
    driver: local
  redis_cluster_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local
  elasticsearch_data:
    driver: local
  scheduler_data:
    driver: local

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Configurações de deploy para produção
x-deploy-defaults: &deploy-defaults
  restart_policy:
    condition: on-failure
    delay: 5s
    max_attempts: 3
    window: 120s
  update_config:
    parallelism: 1
    delay: 10s
    failure_action: rollback
    monitor: 60s
    max_failure_ratio: 0.3
  rollback_config:
    parallelism: 1
    delay: 5s
    failure_action: pause
    monitor: 60s