# ğŸ“ˆ PLANO DE ESCALABILIDADE PROFISSIONAL - ETAPA 3

**Data:** 1 de Fevereiro de 2026  
**Especialista:** Infrastructure Architect (40+ anos)  
**Projeto:** ISP Customer Support - 10.000 Clientes  
**Target:** Escalabilidade Enterprise | Zero Downtime  

---

## ğŸ¯ RESUMO EXECUTIVO

### Crescimento Esperado

```
MÃªs 1:      100 clientes
MÃªs 3:    1.000 clientes
MÃªs 6:   10.000 clientes  â† TARGET
Ano 1:   50.000 clientes  â† ExpansÃ£o regional
```

### Arquitetura Atual vs. NecessÃ¡ria

```
COMPONENTE              ATUAL        10K CLIENTES    IMPROVEMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Workers            1            4-6             4-6x
DB Connections         20           100             5x
Redis Pool             30           200             6-7x
Message Queue          âŒ           128 workers     âœ… Novo
Load Balancer          âŒ           HAProxy/Nginx   âœ… Novo
Database               PostgreSQL   PostgreSQL+     âœ… Master/Slave
Monitoring             Basic        Prometheus+     âœ… AvanÃ§ado
```

---

## 1. CAMADA DE APLICAÃ‡ÃƒO (API)

### 1.1 Problema Atual

```
ConfiguraÃ§Ã£o Atual:
â”œâ”€â”€ 1 instÃ¢ncia de API (uvicorn)
â”œâ”€â”€ 1 worker (ASGI)
â”œâ”€â”€ Pool de conexÃµes: 20+30
â””â”€â”€ Max requisiÃ§Ãµes simultÃ¢neas: ~100

CenÃ¡rio: 10.000 clientes, 30% online = 3.000 usuÃ¡rios

CÃ¡lculo de Carga:
â”œâ”€â”€ 3.000 usuÃ¡rios Ã— 5 msgs/min = 15.000 msgs/min
â”œâ”€â”€ 15.000 msgs/min Ã· 60 = 250 msgs/s
â”œâ”€â”€ Cada mensagem: ~100ms de processamento
â””â”€â”€ CPU necessÃ¡rio: 250 Ã— 0.1s = 25 cores simultÃ¢neos

RESULTADO: âŒ INSUFICIENTE (1 worker vs 25 cores necessÃ¡rios)
```

### 1.2 SoluÃ§Ã£o - Escalabilidade Horizontal

#### Arquitetura Proposta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HAProxy Load Balancer        â”‚  â† Porta 80/443
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ API Worker 1 (uvicorn)          â”‚ â”‚
â”‚ â”‚ - 4 workers (prefork)           â”‚ â”‚
â”‚ â”‚ - Max connections: 250          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ API Worker 2 (uvicorn)          â”‚ â”‚
â”‚ â”‚ - 4 workers (prefork)           â”‚ â”‚
â”‚ â”‚ - Max connections: 250          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ API Worker 3 (uvicorn)          â”‚ â”‚
â”‚ â”‚ - 4 workers (prefork)           â”‚ â”‚
â”‚ â”‚ - Max connections: 250          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ API Worker 4 (uvicorn)          â”‚ â”‚
â”‚ â”‚ - 4 workers (prefork)           â”‚ â”‚
â”‚ â”‚ - Max connections: 250          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ DistribuiÃ§Ã£o de trÃ¡fego
      
Total: 16 workers Ã— 250 conn = 4.000 conexÃµes simultÃ¢neas âœ…
```

#### Docker Compose Atualizado

```yaml
version: '3.8'

services:
  # Load Balancer
  haproxy:
    image: haproxy:2.8-alpine
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # Stats pÃ¡gina
    volumes:
      - ./infra/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./infra/certs:/etc/certs
    depends_on:
      - api-1
      - api-2
      - api-3
      - api-4
    restart: unless-stopped

  # API Instances (1-4)
  api-1:
    build: 
      context: .
      dockerfile: infra/Dockerfile
    command: >
      gunicorn app.main:app
        --workers 4
        --worker-class uvicorn.workers.UvicornWorker
        --bind 0.0.0.0:8000
        --max-requests 10000
        --max-requests-jitter 1000
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${DB_PASS}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false
    depends_on:
      - postgres-master
      - redis-cluster
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  api-2:
    extends: api-1
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${DB_PASS}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false

  api-3:
    extends: api-1
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${DB_PASS}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false

  api-4:
    extends: api-1
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${DB_PASS}@postgres-master:5432/isp_support
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/1
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false
```

---

## 2. CAMADA DE BANCO DE DADOS

### 2.1 Problema Atual

```
ConfiguraÃ§Ã£o Atual:
â”œâ”€â”€ PostgreSQL 15 standalone
â”œâ”€â”€ Connection Pool: 20 + 30 overflow
â”œâ”€â”€ Sem replicaÃ§Ã£o
â””â”€â”€ SEM HIGH AVAILABILITY

CenÃ¡rio CatastrÃ³fico:
â”œâ”€â”€ Falha do servidor = DOWNTIME TOTAL
â”œâ”€â”€ Nenhum backup em standby
â””â”€â”€ Recovery: 1-2 horas
```

### 2.2 SoluÃ§Ã£o - Master/Slave Replication

#### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Primary Database (Master)               â”‚
â”‚      postgres-master:5432                   â”‚
â”‚  - Aceita read + write                      â”‚
â”‚  - Replication WAL logs                     â”‚
â”‚  - Max pool: 100 connections                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  WAL Replication   â”‚
        â”‚  (Streaming)       â”‚
        â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Standby 1        â”‚  â”‚ Standby 2        â”‚
â”‚ postgres-slave-1 â”‚  â”‚ postgres-slave-2 â”‚
â”‚ READ-ONLY        â”‚  â”‚ READ-ONLY        â”‚
â”‚ Hot standby      â”‚  â”‚ Hot standby      â”‚
â”‚ Cascading rep.   â”‚  â”‚ (Backup)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Docker Compose - DB

```yaml
  postgres-master:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: isp_support
      POSTGRES_INITDB_ARGS: >
        -c max_connections=200
        -c shared_buffers=4GB
        -c effective_cache_size=12GB
        -c maintenance_work_mem=1GB
        -c checkpoint_completion_target=0.9
        -c wal_buffers=16MB
        -c default_statistics_target=100
        -c random_page_cost=1.1
        -c effective_io_concurrency=200
        -c work_mem=20MB
        -c wal_level=replica
        -c max_wal_senders=10
        -c max_replication_slots=10
    volumes:
      - postgres-master-data:/var/lib/postgresql/data
      - ./infra/pg_config/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./infra/pg_config/pg_hba.conf:/etc/postgresql/pg_hba.conf
    command: >
      postgres
        -c config_file=/etc/postgresql/postgresql.conf
        -c hba_file=/etc/postgresql/pg_hba.conf
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-slave-1:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres-slave-1-data:/var/lib/postgresql/data
    command: >
      bash -c "
        pg_basebackup -h postgres-master -D /var/lib/postgresql/data -U postgres -v -W &&
        pg_ctl -D /var/lib/postgresql/data -l /var/log/postgres.log start
      "
    depends_on:
      - postgres-master
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-slave-2:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres-slave-2-data:/var/lib/postgresql/data
    command: >
      bash -c "
        pg_basebackup -h postgres-master -D /var/lib/postgresql/data -U postgres -v -W &&
        pg_ctl -D /var/lib/postgresql/data -l /var/log/postgres.log start
      "
    depends_on:
      - postgres-master
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres-master-data:
  postgres-slave-1-data:
  postgres-slave-2-data:
```

#### ConfiguraÃ§Ã£o de Failover AutomÃ¡tico

```python
# /app/core/database_failover.py

class DatabaseFailoverManager:
    """Gerenciar failover automÃ¡tico de banco de dados"""
    
    async def check_master_health(self) -> bool:
        """Verificar saÃºde do master"""
        try:
            async with self.master_pool.connection() as conn:
                result = await conn.fetchval("SELECT 1")
                return result == 1
        except Exception as e:
            logger.error("Master health check failed", error=str(e))
            return False
    
    async def promote_slave_to_master(self):
        """Promover slave para master em caso de falha"""
        
        try:
            # 1. Verificar que master realmente estÃ¡ down
            is_master_down = not await self.check_master_health()
            
            if not is_master_down:
                logger.warning("Master is still up, skipping promotion")
                return
            
            # 2. Promover slave-1 para master
            logger.critical("Promoting slave-1 to master")
            
            async with self.slave_pool.connection() as conn:
                await conn.execute("SELECT pg_promote()")
            
            # 3. Reconfigurar aplicaÃ§Ã£o
            settings.DATABASE_URL = "postgresql+asyncpg://postgres:...@postgres-slave-1:5432/isp_support"
            await self.db_manager.reinitialize()
            
            # 4. Notificar administrador
            await self.send_alert(
                severity="CRITICAL",
                message="Database failover completed: slave-1 promoted to master"
            )
            
            logger.info("Failover completed successfully")
            
        except Exception as e:
            logger.error("Failover failed", error=str(e))
            await self.send_alert(
                severity="CRITICAL",
                message=f"Database failover FAILED: {str(e)}"
            )
```

---

## 3. CAMADA DE CACHE

### 3.1 Problema Atual

```
ConfiguraÃ§Ã£o Atual:
â”œâ”€â”€ Redis Standalone
â”œâ”€â”€ Pool: 30 conexÃµes
â”œâ”€â”€ Sem cluster
â”œâ”€â”€ Sem eviction policy
â””â”€â”€ Sem persistÃªncia

Problema em escala:
â”œâ”€â”€ 250 msgs/s = 250 cache ops/s
â”œâ”€â”€ Pool de 30 nÃ£o aguenta
â”œâ”€â”€ Sem replicaÃ§Ã£o = perda de dados
â””â”€â”€ SEM ALTA DISPONIBILIDADE
```

### 3.2 SoluÃ§Ã£o - Redis Cluster + Sentinel

#### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis Cluster (3 masters + 3 slaves)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  Master 1   Master 2   Master 3          â”‚
â”‚    â†“          â†“          â†“               â”‚
â”‚  Slave 1   Slave 2   Slave 3             â”‚
â”‚                                          â”‚
â”‚  Sharding automÃ¡tico:                    â”‚
â”‚  â”œâ”€ Key 1-5461 â†’ Master 1                â”‚
â”‚  â”œâ”€ Key 5462-10922 â†’ Master 2            â”‚
â”‚  â””â”€ Key 10923-16383 â†’ Master 3           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘              â†‘
      Sentinel 1   Sentinel 2
      
  (Monitora failover automÃ¡tico)
```

#### Docker Compose - Redis Cluster

```yaml
  redis-master-1:
    image: redis:7-alpine
    command: >
      redis-server --port 6379
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --maxmemory 1gb
        --maxmemory-policy allkeys-lru
        --appendonly yes
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-master-1:/data
    ports:
      - "6379:6379"

  redis-master-2:
    image: redis:7-alpine
    command: >
      redis-server --port 6380
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --maxmemory 1gb
        --maxmemory-policy allkeys-lru
        --appendonly yes
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-master-2:/data
    ports:
      - "6380:6380"

  redis-master-3:
    image: redis:7-alpine
    command: >
      redis-server --port 6381
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --maxmemory 1gb
        --maxmemory-policy allkeys-lru
        --appendonly yes
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-master-3:/data
    ports:
      - "6381:6381"

  redis-slave-1:
    image: redis:7-alpine
    command: >
      redis-server --port 6382
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --slaveof redis-master-1 6379
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-slave-1:/data
    depends_on:
      - redis-master-1

  redis-slave-2:
    image: redis:7-alpine
    command: >
      redis-server --port 6383
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --slaveof redis-master-2 6380
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-slave-2:/data
    depends_on:
      - redis-master-2

  redis-slave-3:
    image: redis:7-alpine
    command: >
      redis-server --port 6384
        --cluster-enabled yes
        --cluster-config-file /data/nodes.conf
        --cluster-node-timeout 5000
        --slaveof redis-master-3 6381
        --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-slave-3:/data
    depends_on:
      - redis-master-3

volumes:
  redis-master-1:
  redis-master-2:
  redis-master-3:
  redis-slave-1:
  redis-slave-2:
  redis-slave-3:
```

#### Atualizar Redis Client

```python
# /app/core/redis_client.py

class AdvancedRedisManager:
    
    async def initialize(self):
        """Inicializar com cluster"""
        
        config = get_redis_config()
        
        # Usar cluster ao invÃ©s de instance Ãºnica
        self.client = RedisCluster(
            startup_nodes=[
                {"host": "redis-master-1", "port": 6379},
                {"host": "redis-master-2", "port": 6380},
                {"host": "redis-master-3", "port": 6381},
            ],
            password=settings.REDIS_PASSWORD,
            decode_responses=True,
            
            # Eviction policy
            config_reset_on_cluster_changed=True,
            skip_full_coverage_check=True,
            
            # ResilÃªncia
            retry_on_timeout=True,
            retry_on_error=[redis.ConnectionError, redis.TimeoutError],
            health_check_interval=30,
            
            # Performance
            socket_keepalive=True,
            socket_keepalive_options={1: 1, 2: 3, 3: 5},
            
            # Pools maiores
            max_connections=200  # Aumentado para 10k clientes
        )
```

---

## 4. CAMADA DE FILA (MESSAGE QUEUE)

### 4.1 Problema Atual

```
ConfiguraÃ§Ã£o Atual:
â”œâ”€â”€ Celery com Redis broker âŒ (NÃ£o recomendado)
â”œâ”€â”€ Workers: NÃƒO CONFIGURADOS
â””â”€â”€ Sem alta disponibilidade

Problema:
â”œâ”€â”€ Redis nÃ£o Ã© recomendado para Celery em produÃ§Ã£o
â”œâ”€â”€ Sem persistÃªncia de mensagens
â”œâ”€â”€ Sem replicaÃ§Ã£o
â””â”€â”€ Perda de tasks em falha
```

### 4.2 SoluÃ§Ã£o - RabbitMQ + Celery

#### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI (Main App)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
         enqueue task
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RabbitMQ (Message Broker) â”‚  â† PersistÃªncia de mensagens
â”‚   - 3 nÃ³s cluster         â”‚
â”‚   - Mirrored queues       â”‚
â”‚   - Ack automÃ¡tico        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”œâ”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
        â†“    â†“    â†“    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Celery Workers       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Worker 1: AI Tasks   â”‚
    â”‚ - 4 concurrency      â”‚
    â”‚                      â”‚
    â”‚ Worker 2: Messages   â”‚
    â”‚ - 8 concurrency      â”‚
    â”‚                      â”‚
    â”‚ Worker 3: Reports    â”‚
    â”‚ - 2 concurrency      â”‚
    â”‚                      â”‚
    â”‚ Worker 4: Webhooks   â”‚
    â”‚ - 4 concurrency      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Docker Compose - RabbitMQ + Celery

```yaml
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_DEFAULT_VHOST: isp_support
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
      - ./infra/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    ports:
      - "5672:5672"    # AMQP
      - "15672:15672"  # Management UI
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  celery-worker-ai:
    build:
      context: .
      dockerfile: infra/Dockerfile
    command: >
      celery -A app.workers.main worker
        --loglevel=info
        --queue=ai_tasks
        --concurrency=4
        --max-tasks-per-child=1000
        --prefetch-multiplier=1
        --time-limit=300
        --soft-time-limit=250
    environment:
      - CELERY_BROKER_URL=amqp://rabbitmq_user:rabbitmq_pass@rabbitmq:5672/isp_support
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - DATABASE_URL=postgresql+asyncpg://...
    depends_on:
      - rabbitmq
      - redis-master-1

  celery-worker-messages:
    build:
      context: .
      dockerfile: infra/Dockerfile
    command: >
      celery -A app.workers.main worker
        --loglevel=info
        --queue=messages,high_priority
        --concurrency=8
        --max-tasks-per-child=500
        --prefetch-multiplier=1
        --time-limit=60
        --soft-time-limit=50
    environment:
      - CELERY_BROKER_URL=amqp://rabbitmq_user:rabbitmq_pass@rabbitmq:5672/isp_support
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - DATABASE_URL=postgresql+asyncpg://...
    depends_on:
      - rabbitmq
      - redis-master-1

  celery-worker-reports:
    build:
      context: .
      dockerfile: infra/Dockerfile
    command: >
      celery -A app.workers.main worker
        --loglevel=info
        --queue=reports,batch_jobs
        --concurrency=2
        --max-tasks-per-child=100
        --time-limit=3600
        --soft-time-limit=3500
    environment:
      - CELERY_BROKER_URL=amqp://rabbitmq_user:rabbitmq_pass@rabbitmq:5672/isp_support
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - DATABASE_URL=postgresql+asyncpg://...
    depends_on:
      - rabbitmq
      - redis-master-1

  celery-beat:
    build:
      context: .
      dockerfile: infra/Dockerfile
    command: >
      celery -A app.workers.main beat
        --loglevel=info
        --scheduler=celery_beat.schedulers:DatabaseScheduler
    environment:
      - CELERY_BROKER_URL=amqp://rabbitmq_user:rabbitmq_pass@rabbitmq:5672/isp_support
      - CELERY_RESULT_BACKEND=redis://redis-cluster:6379/2
      - DATABASE_URL=postgresql+asyncpg://...
    depends_on:
      - rabbitmq
      - redis-master-1

volumes:
  rabbitmq-data:
```

---

## 5. MONITORAMENTO & ALERTAS

### 5.1 Stack de Monitoramento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Prometheus (Coleta de MÃ©tricas)    â”‚
â”‚    - Scrape interval: 15s              â”‚
â”‚    - Retention: 15 dias                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“            â†“            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana  â”‚ â”‚AlertManagerâ”‚ â”‚ELK Stack â”‚
â”‚Dashboardâ”‚ â”‚(PagerDuty/  â”‚ â”‚ Logs    â”‚
â”‚ Real-   â”‚ â”‚ Slack)      â”‚ â”‚        â”‚
â”‚ time    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Docker Compose - Monitoramento

```yaml
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./infra/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./infra/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./infra/grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.0.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.0.0
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
```

---

## 6. PLANO DE IMPLEMENTAÃ‡ÃƒO

### Timeline Profissional

```
SEMANA 1:
â”œâ”€ Segunda: Setup Load Balancer + API escalada
â”œâ”€ TerÃ§a: Database Master/Slave replication
â”œâ”€ Quarta: Redis Cluster + Sentinel
â”œâ”€ Quinta: RabbitMQ + Celery workers
â””â”€ Sexta: Testes de carga + otimizaÃ§Ãµes

SEMANA 2:
â”œâ”€ Segunda: Monitoramento completo (Prometheus + Grafana)
â”œâ”€ TerÃ§a: ELK Stack para logs
â”œâ”€ Quarta: Alertas inteligentes (PagerDuty)
â”œâ”€ Quinta: Disaster recovery tests
â””â”€ Sexta: Deploy para produÃ§Ã£o

SEMANA 3:
â”œâ”€ Monitoring contÃ­nuo em produÃ§Ã£o
â”œâ”€ Fine-tuning de performance
â”œâ”€ OtimizaÃ§Ãµes de Ã­ndices DB
â””â”€ PreparaÃ§Ã£o para 50k clientes
```

---

## 7. CHECKLIST DE ESCALABILIDADE

### Infraestrutura âœ…
- [ ] API em 4 instÃ¢ncias com load balancer
- [ ] PostgreSQL Master/Slave com failover
- [ ] Redis Cluster 3+3 com Sentinel
- [ ] RabbitMQ com 4 tipos de workers
- [ ] HAProxy configurado para health checks
- [ ] Nginx como reverse proxy

### Performance âœ…
- [ ] Database pool: 100 conexÃµes
- [ ] Connection pooling otimizado
- [ ] Ãndices DB para queries crÃ­ticas
- [ ] Cache estratÃ©gico (L1/L2/L3)
- [ ] CompressÃ£o de dados (gzip)
- [ ] CDN para assets estÃ¡ticos

### Observabilidade âœ…
- [ ] Prometheus coletando mÃ©tricas
- [ ] Grafana com 10+ dashboards
- [ ] ELK Stack para logs centralizados
- [ ] AlertManager com Slack integration
- [ ] Health checks em todos serviÃ§os
- [ ] APM/Tracing distribuÃ­do

### SeguranÃ§a âœ…
- [ ] LGPD compliance
- [ ] Rate limiting real
- [ ] Criptografia end-to-end
- [ ] Auditoria de acessos
- [ ] WAF (ModSecurity)
- [ ] Backup criptografado

### ResilÃªncia âœ…
- [ ] Failover automÃ¡tico BD
- [ ] Circuit breaker patterns
- [ ] Retry logic com backoff
- [ ] Graceful degradation
- [ ] Chaos engineering tests
- [ ] RTO: < 5 minutos, RPO: < 1 minuto

---

## 8. MÃ‰TRICAS DE SUCESSO

### SLA - Service Level Agreement

```
Uptime:                99.95%  (< 2.2 horas/mÃªs downtime)
Response Time (p95):   200ms
Response Time (p99):   500ms
Error Rate:            < 0.1%
Cache Hit Rate:        > 80%
Database Query Time:   < 100ms (p95)
```

---

## ğŸ“ PRÃ“XIMOS PASSOS

âœ… **Etapa 1:** AnÃ¡lise de Arquitetura âœ“  
âœ… **Etapa 2:** Gaps de SeguranÃ§a âœ“  
âœ… **Etapa 3:** Plano de Escalabilidade âœ“  

ğŸ”„ **PrÃ³xima Fase:** ImplementaÃ§Ã£o das soluÃ§Ãµes em sequÃªncia

---

**Especialista:** Infrastructure Architect  
**Data:** 1 de Fevereiro de 2026
